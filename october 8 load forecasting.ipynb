{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MS7seqQbmwnc",
        "outputId": "b53e33fd-6cc3-4bff-ff67-eb783ed8b7dd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: openpyxl in /usr/local/lib/python3.12/dist-packages (3.1.5)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: et-xmlfile in /usr/local/lib/python3.12/dist-packages (from openpyxl) (2.0.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: holidays in /usr/local/lib/python3.12/dist-packages (0.81)\n",
            "Requirement already satisfied: tensorflow in /usr/local/lib/python3.12/dist-packages (2.19.0)\n",
            "Requirement already satisfied: xlrd in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.12/dist-packages (from holidays) (2.9.0.post0)\n",
            "Requirement already satisfied: absl-py>=1.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.4.0)\n",
            "Requirement already satisfied: astunparse>=1.6.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.6.3)\n",
            "Requirement already satisfied: flatbuffers>=24.3.25 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (25.9.23)\n",
            "Requirement already satisfied: gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.6.0)\n",
            "Requirement already satisfied: google-pasta>=0.1.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.2.0)\n",
            "Requirement already satisfied: libclang>=13.0.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (18.1.1)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.4.0)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.12/dist-packages (from tensorflow) (25.0)\n",
            "Requirement already satisfied: protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (5.29.5)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.32.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from tensorflow) (75.2.0)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.17.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.1.0)\n",
            "Requirement already satisfied: typing-extensions>=3.6.6 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (4.15.0)\n",
            "Requirement already satisfied: wrapt>=1.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.17.3)\n",
            "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (1.75.1)\n",
            "Requirement already satisfied: tensorboard~=2.19.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.19.0)\n",
            "Requirement already satisfied: keras>=3.5.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.10.0)\n",
            "Requirement already satisfied: numpy<2.2.0,>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (2.0.2)\n",
            "Requirement already satisfied: h5py>=3.11.0 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (3.14.0)\n",
            "Requirement already satisfied: ml-dtypes<1.0.0,>=0.5.1 in /usr/local/lib/python3.12/dist-packages (from tensorflow) (0.5.3)\n",
            "Requirement already satisfied: wheel<1.0,>=0.23.0 in /usr/local/lib/python3.12/dist-packages (from astunparse>=1.6.0->tensorflow) (0.45.1)\n",
            "Requirement already satisfied: rich in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (13.9.4)\n",
            "Requirement already satisfied: namex in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (0.1.0)\n",
            "Requirement already satisfied: optree in /usr/local/lib/python3.12/dist-packages (from keras>=3.5.0->tensorflow) (0.17.0)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.4.3)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests<3,>=2.21.0->tensorflow) (2025.8.3)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.9)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.12/dist-packages (from tensorboard~=2.19.0->tensorflow) (3.1.3)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.12/dist-packages (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow) (3.0.3)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow) (4.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.12/dist-packages (from rich->keras>=3.5.0->tensorflow) (2.19.2)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.12/dist-packages (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow) (0.1.2)\n",
            "Combined shape: (184082, 9)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/openpyxl/styles/stylesheet.py:237: UserWarning: Workbook contains no default style, apply openpyxl's default\n",
            "  warn(\"Workbook contains no default style, apply openpyxl's default\")\n",
            "/usr/local/lib/python3.12/dist-packages/openpyxl/styles/stylesheet.py:237: UserWarning: Workbook contains no default style, apply openpyxl's default\n",
            "  warn(\"Workbook contains no default style, apply openpyxl's default\")\n",
            "/usr/local/lib/python3.12/dist-packages/openpyxl/styles/stylesheet.py:237: UserWarning: Workbook contains no default style, apply openpyxl's default\n",
            "  warn(\"Workbook contains no default style, apply openpyxl's default\")\n",
            "/usr/local/lib/python3.12/dist-packages/openpyxl/styles/stylesheet.py:237: UserWarning: Workbook contains no default style, apply openpyxl's default\n",
            "  warn(\"Workbook contains no default style, apply openpyxl's default\")\n",
            "/usr/local/lib/python3.12/dist-packages/openpyxl/styles/stylesheet.py:237: UserWarning: Workbook contains no default style, apply openpyxl's default\n",
            "  warn(\"Workbook contains no default style, apply openpyxl's default\")\n"
          ]
        }
      ],
      "source": [
        "!pip install pandas openpyxl\n",
        "!pip install holidays tensorflow xlrd\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import holidays\n",
        "import os\n",
        "import datetime\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense\n",
        "from datetime import datetime, timedelta\n",
        "from tensorflow.keras.layers import Input\n",
        "\n",
        "df_list = []\n",
        "\n",
        "for year in range(2004, 2025):\n",
        "\n",
        "  base_file = f'Native_Load_{year}'\n",
        "  file_xlsx = base_file + '.xlsx'\n",
        "  file_xls = base_file + '.xls'\n",
        "\n",
        "  if os.path.exists(file_xlsx):\n",
        "      df_year = pd.read_excel(file_xlsx, engine='openpyxl')\n",
        "      file_name = file_xlsx\n",
        "  elif os.path.exists(file_xls):\n",
        "      df_year = pd.read_excel(file_xls)\n",
        "      file_name = file_xls\n",
        "  else:\n",
        "        print(f\"file for the year {year} not found.\")\n",
        "        continue\n",
        "\n",
        "  df_year.rename(columns={df_year.columns[0]: 'timestamp'}, inplace=True)\n",
        "  df_year = df_year[['timestamp', 'SCENT']].copy() # keeps only south central region + timestamp\n",
        "  df_year.rename(columns={'SCENT': 'load'}, inplace=True) # rename SCENT --> load\n",
        "\n",
        "  def fix_24_hour(ts):\n",
        "      if '24:00' in ts:\n",
        "          date_part = ts.split()[0]\n",
        "          fixed_ts = pd.to_datetime(date_part) + pd.Timedelta(days=1)\n",
        "          return fixed_ts.replace(hour=0, minute=0)\n",
        "      return pd.to_datetime(ts)\n",
        "  df_year['timestamp'] = df_year['timestamp'].astype(str).apply(fix_24_hour)\n",
        "  df_year['timestamp'] = df_year['timestamp'].dt.round('h')\n",
        "\n",
        "  # create the hour, day of week, month, and is_weekend\n",
        "  df_year['hour'] = df_year['timestamp'].dt.hour\n",
        "  df_year['dayofweek'] = df_year['timestamp'].dt.dayofweek\n",
        "  df_year['day'] = df_year['timestamp'].dt.day\n",
        "  df_year['year'] = df_year['timestamp'].dt.year\n",
        "  df_year['month'] = df_year['timestamp'].dt.month\n",
        "  df_year['is_weekend'] = df_year['dayofweek'] >= 5\n",
        "\n",
        "  # check for holidays\n",
        "  us_holidays = holidays.US(years=year)\n",
        "  df_year['is_holiday'] = df_year['timestamp'].dt.date.isin(us_holidays)\n",
        "  # drop daylight savings dupes\n",
        "  df_year = df_year[~df_year['timestamp'].duplicated()]\n",
        "\n",
        "  df_list.append(df_year)\n",
        "\n",
        "df = pd.concat(df_list, ignore_index=True)\n",
        "print(\"Combined shape:\", df.shape)\n",
        "\n",
        "temp_list = []\n",
        "\n",
        "for file in [\"austin, texas 2004-01-01 to 2006-12-31.xlsx\", \"austin, texas 2007-01-01 to 2009-12-31.xlsx\", \"austin, texas 2010-01-01 to 2012-12-31.xlsx\", \"austin, texas 2013-01-01 to 2016-12-31.xlsx\", \"austin, texas 2017-01-01 to 2020-12-31.xlsx\", \"austin, texas 2021-01-01 to 2024-12-31.xlsx\"]:\n",
        "    temp_df = pd.read_excel(file)\n",
        "    temp_df.rename(columns={temp_df.columns[0]: 'timestamp'}, inplace=True)\n",
        "    temp_df['timestamp'] = pd.to_datetime(temp_df['timestamp']).dt.round('h')\n",
        "    temp_df.rename(columns={temp_df.columns[1]: 'temperature'}, inplace=True)\n",
        "    temp_list.append(temp_df)\n",
        "\n",
        "df_temp = pd.concat(temp_list, ignore_index=True)\n",
        "df = pd.merge(df, df_temp, on='timestamp', how='left')\n",
        "df['temperature'] = df['temperature'].ffill()\n",
        "\n",
        "# check num of rows\n",
        "# df.head(30)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "NvqmGlEtylq9"
      },
      "outputs": [],
      "source": [
        "# remove rows with NaNs after lagging\n",
        "df = df.dropna()\n",
        "\n",
        "#'temperature',\n",
        "features = ['hour', 'day', 'dayofweek', 'month','is_weekend', 'is_holiday', 'year', 'temperature']\n",
        "\n",
        "scaler_X = StandardScaler()\n",
        "X = df[features]\n",
        "y = df['load']\n",
        "X = X[~y.isna()]\n",
        "y = y.dropna()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pVCx9oHRyYQ7",
        "outputId": "89188aa2-ca5c-4049-cbb8-7788c41cf845"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Any NaN in X_train?  False\n",
            "Any infinite in X_train?  False\n",
            "Any NaN in y_train?  False\n",
            "Any infinite in y_train?  False\n",
            "Total NaNs in y: 0 out of 184102 samples\n",
            "Epoch 1/30\n",
            "\u001b[1m4143/4143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 3ms/step - loss: 3914952.0000 - mae: 1027.5165 - val_loss: 315980.6250 - val_mae: 428.8545 - learning_rate: 0.0010\n",
            "Epoch 2/30\n",
            "\u001b[1m4143/4143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 3ms/step - loss: 279045.3125 - mae: 400.0164 - val_loss: 203223.3281 - val_mae: 339.6407 - learning_rate: 0.0010\n",
            "Epoch 3/30\n",
            "\u001b[1m4143/4143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 3ms/step - loss: 203485.2031 - mae: 339.1553 - val_loss: 201006.3906 - val_mae: 336.9250 - learning_rate: 0.0010\n",
            "Epoch 4/30\n",
            "\u001b[1m4143/4143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 3ms/step - loss: 184793.8750 - mae: 321.6414 - val_loss: 173716.7500 - val_mae: 310.8599 - learning_rate: 0.0010\n",
            "Epoch 5/30\n",
            "\u001b[1m4143/4143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 2ms/step - loss: 171577.1250 - mae: 308.5878 - val_loss: 166043.0938 - val_mae: 304.5573 - learning_rate: 0.0010\n",
            "Epoch 6/30\n",
            "\u001b[1m4143/4143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 3ms/step - loss: 158712.0781 - mae: 296.5914 - val_loss: 153188.2500 - val_mae: 296.0842 - learning_rate: 0.0010\n",
            "Epoch 7/30\n",
            "\u001b[1m4143/4143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 3ms/step - loss: 154019.5781 - mae: 290.0327 - val_loss: 154585.9688 - val_mae: 294.8337 - learning_rate: 0.0010\n",
            "Epoch 8/30\n",
            "\u001b[1m4143/4143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 3ms/step - loss: 149385.0938 - mae: 286.2266 - val_loss: 137458.7344 - val_mae: 274.0252 - learning_rate: 0.0010\n",
            "Epoch 9/30\n",
            "\u001b[1m4143/4143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 3ms/step - loss: 148322.6562 - mae: 283.9385 - val_loss: 167597.0938 - val_mae: 313.0546 - learning_rate: 0.0010\n",
            "Epoch 10/30\n",
            "\u001b[1m4143/4143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 3ms/step - loss: 144437.3594 - mae: 280.7809 - val_loss: 146564.0938 - val_mae: 283.0206 - learning_rate: 0.0010\n",
            "Epoch 11/30\n",
            "\u001b[1m4143/4143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 2ms/step - loss: 135043.4219 - mae: 269.6604 - val_loss: 133848.0156 - val_mae: 269.0824 - learning_rate: 5.0000e-04\n",
            "Epoch 12/30\n",
            "\u001b[1m4143/4143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 3ms/step - loss: 134302.6875 - mae: 267.9139 - val_loss: 130532.1562 - val_mae: 264.5653 - learning_rate: 5.0000e-04\n",
            "Epoch 13/30\n",
            "\u001b[1m4143/4143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 3ms/step - loss: 133522.9375 - mae: 267.7542 - val_loss: 130942.3281 - val_mae: 265.3981 - learning_rate: 5.0000e-04\n",
            "Epoch 14/30\n",
            "\u001b[1m4143/4143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 3ms/step - loss: 130413.6016 - mae: 265.1529 - val_loss: 132079.5781 - val_mae: 269.3745 - learning_rate: 5.0000e-04\n",
            "Epoch 15/30\n",
            "\u001b[1m4143/4143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 3ms/step - loss: 125577.7500 - mae: 257.9842 - val_loss: 127225.7500 - val_mae: 263.6662 - learning_rate: 2.5000e-04\n",
            "Epoch 16/30\n",
            "\u001b[1m4143/4143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 2ms/step - loss: 124986.3750 - mae: 257.9063 - val_loss: 123143.9531 - val_mae: 256.8987 - learning_rate: 2.5000e-04\n",
            "Epoch 17/30\n",
            "\u001b[1m4143/4143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 2ms/step - loss: 123138.5234 - mae: 255.9737 - val_loss: 125697.3203 - val_mae: 259.8606 - learning_rate: 2.5000e-04\n",
            "Epoch 18/30\n",
            "\u001b[1m4143/4143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 3ms/step - loss: 123197.8906 - mae: 255.9140 - val_loss: 121836.8203 - val_mae: 256.7723 - learning_rate: 2.5000e-04\n",
            "Epoch 19/30\n",
            "\u001b[1m4143/4143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 3ms/step - loss: 122775.6484 - mae: 255.0055 - val_loss: 123120.0938 - val_mae: 257.3253 - learning_rate: 2.5000e-04\n",
            "Epoch 20/30\n",
            "\u001b[1m4143/4143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 3ms/step - loss: 124189.8359 - mae: 255.1687 - val_loss: 126184.0234 - val_mae: 262.8410 - learning_rate: 2.5000e-04\n",
            "Epoch 21/30\n",
            "\u001b[1m4143/4143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 3ms/step - loss: 118951.3516 - mae: 250.9101 - val_loss: 119496.0156 - val_mae: 252.0732 - learning_rate: 1.2500e-04\n",
            "Epoch 22/30\n",
            "\u001b[1m4143/4143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 2ms/step - loss: 117817.2266 - mae: 248.3681 - val_loss: 119411.5625 - val_mae: 253.4516 - learning_rate: 1.2500e-04\n",
            "Epoch 23/30\n",
            "\u001b[1m4143/4143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 3ms/step - loss: 117272.2812 - mae: 249.0255 - val_loss: 119683.7734 - val_mae: 252.1780 - learning_rate: 1.2500e-04\n",
            "Epoch 24/30\n",
            "\u001b[1m4143/4143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 3ms/step - loss: 118022.1797 - mae: 249.1958 - val_loss: 119075.7266 - val_mae: 251.8678 - learning_rate: 1.2500e-04\n",
            "Epoch 25/30\n",
            "\u001b[1m4143/4143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 3ms/step - loss: 115491.2891 - mae: 248.1022 - val_loss: 118597.5078 - val_mae: 252.2296 - learning_rate: 1.2500e-04\n",
            "Epoch 26/30\n",
            "\u001b[1m4143/4143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 3ms/step - loss: 115432.3828 - mae: 247.0788 - val_loss: 117431.8516 - val_mae: 250.8296 - learning_rate: 1.2500e-04\n",
            "Epoch 27/30\n",
            "\u001b[1m4143/4143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 2ms/step - loss: 115432.8047 - mae: 246.8264 - val_loss: 119355.8047 - val_mae: 253.0211 - learning_rate: 1.2500e-04\n",
            "Epoch 28/30\n",
            "\u001b[1m4143/4143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 3ms/step - loss: 116446.6406 - mae: 247.4749 - val_loss: 118198.8516 - val_mae: 252.3316 - learning_rate: 1.2500e-04\n",
            "Epoch 29/30\n",
            "\u001b[1m4143/4143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 3ms/step - loss: 114045.0547 - mae: 245.5888 - val_loss: 117293.3750 - val_mae: 250.8658 - learning_rate: 6.2500e-05\n",
            "Epoch 30/30\n",
            "\u001b[1m4143/4143\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m21s\u001b[0m 3ms/step - loss: 112298.3828 - mae: 242.9086 - val_loss: 115530.3750 - val_mae: 247.7222 - learning_rate: 6.2500e-05\n",
            "\u001b[1m1151/1151\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 1ms/step\n",
            "MAPE on test set: 3.79%\n"
          ]
        }
      ],
      "source": [
        "scaler = StandardScaler()\n",
        "X_scaled = scaler.fit_transform(X)\n",
        "X_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.2, random_state=42)\n",
        "\n",
        "print(\"Any NaN in X_train? \", pd.isna(X_train).any())\n",
        "print(\"Any infinite in X_train? \", (X_train == float('inf')).any())\n",
        "print(\"Any NaN in y_train? \", y_train.isna().any())\n",
        "print(\"Any infinite in y_train? \", (y_train == float('inf')).any())\n",
        "print(f\"Total NaNs in y: {y.isna().sum()} out of {len(y)} samples\")\n",
        "\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "model = Sequential([\n",
        "    Input(shape=(X_train.shape[1],)),\n",
        "    Dense(256, activation='relu'),\n",
        "    Dense(256, activation='relu'),\n",
        "    Dense(128, activation='relu'),\n",
        "    Dense(64, activation='relu'),\n",
        "    Dense(1)\n",
        "])\n",
        "\n",
        "from tensorflow.keras.callbacks import ReduceLROnPlateau\n",
        "\n",
        "reduce_lr = ReduceLROnPlateau(\n",
        "    monitor='val_loss',  # track validation loss\n",
        "    factor=0.5,          # reduce LR by 50%\n",
        "    patience=2,          # wait 5 epochs without improvement\n",
        "    min_lr=1e-6          # lower bound\n",
        ")\n",
        "\n",
        "opt = Adam(learning_rate=0.001)\n",
        "\n",
        "model.compile(optimizer=opt, loss='mse', metrics=['mae'])\n",
        "\n",
        "model.fit(X_train, y_train, epochs=30, batch_size=32, validation_split=0.1, callbacks=[reduce_lr], verbose=1)\n",
        "\n",
        "# MAPE\n",
        "y_pred = model.predict(X_test).flatten()\n",
        "mape = np.mean(np.abs((y_test - y_pred) / y_test)) * 100\n",
        "print(f\"MAPE on test set: {mape:.2f}%\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tomorrow = (pd.Timestamp.now(tz='US/Pacific').normalize().tz_convert(None)+ pd.Timedelta(days=1))\n",
        "yesterday = tomorrow - pd.Timedelta(days=1)\n",
        "print(f\"Tomorrow's date: {tomorrow.date()}\")\n",
        "tomorrow_month = tomorrow.month\n",
        "tomorrow_day = tomorrow.day\n",
        "\n",
        "tomorrow_rows = []\n",
        "for hour in range(24):\n",
        "    temp_rows = df[(df['month'] == tomorrow_month) & (df['day'] == tomorrow_day) & (df['hour'] == hour)]\n",
        "    temp_tomorrow = temp_rows['temperature'].mean() if not temp_rows.empty else None\n",
        "\n",
        "    dayofweek = tomorrow.dayofweek\n",
        "    day = tomorrow_day\n",
        "    month = tomorrow_month\n",
        "    is_weekend = int(dayofweek >= 5)\n",
        "    is_holiday = int(tomorrow.date() in us_holidays)\n",
        "\n",
        "    tomorrow_rows.append([\n",
        "        hour,\n",
        "        day, dayofweek, month,\n",
        "        is_weekend, is_holiday,\n",
        "        tomorrow.year,\n",
        "        temp_tomorrow\n",
        "    ])\n",
        "\n",
        "X_tomorrow = pd.DataFrame(tomorrow_rows, columns=features)\n",
        "X_tomorrow_scaled = scaler.transform(X_tomorrow)\n",
        "\n",
        "predicted_loads = model.predict(X_tomorrow_scaled).flatten()\n",
        "\n",
        "for hour, load in enumerate(predicted_loads):\n",
        "    print(f\"Hour {hour:02d}: {load:.2f} MW\")\n",
        "\n",
        "# print(tomorrow_df)\n",
        "# df[(df['year'] == 2024) & (df['month'] == 8) & (df['day'] == 16)].dropna()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rTpeDuyJuVLR",
        "outputId": "68dde41a-212d-40b6-eb86-c7f2e0ff00c9"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Tomorrow's date: 2025-10-09\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
            "Hour 00: 7685.09 MW\n",
            "Hour 01: 7156.71 MW\n",
            "Hour 02: 6732.71 MW\n",
            "Hour 03: 6442.01 MW\n",
            "Hour 04: 6328.88 MW\n",
            "Hour 05: 6341.02 MW\n",
            "Hour 06: 6510.87 MW\n",
            "Hour 07: 6838.90 MW\n",
            "Hour 08: 7046.95 MW\n",
            "Hour 09: 7328.21 MW\n",
            "Hour 10: 7572.79 MW\n",
            "Hour 11: 7843.74 MW\n",
            "Hour 12: 8167.21 MW\n",
            "Hour 13: 8484.90 MW\n",
            "Hour 14: 8830.73 MW\n",
            "Hour 15: 9254.69 MW\n",
            "Hour 16: 9659.07 MW\n",
            "Hour 17: 10019.45 MW\n",
            "Hour 18: 10153.84 MW\n",
            "Hour 19: 9991.49 MW\n",
            "Hour 20: 9671.20 MW\n",
            "Hour 21: 9373.93 MW\n",
            "Hour 22: 8919.26 MW\n",
            "Hour 23: 8334.60 MW\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "from datetime import datetime\n",
        "from google.colab import drive\n",
        "\n",
        "# --- Force remount if needed ---\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "\n",
        "# --- Directory and file paths ---\n",
        "save_directory = '/content/drive/MyDrive'\n",
        "one_time_file = os.path.join(save_directory, 'predicted_load.csv')\n",
        "\n",
        "# Ensure directory exists\n",
        "os.makedirs(save_directory, exist_ok=True)\n",
        "\n",
        "# --- Create your DataFrame ---\n",
        "ghi_df = pd.DataFrame({\n",
        "    'hour': list(range(24)),\n",
        "    'predicted_load': predicted_loads\n",
        "})\n",
        "\n",
        "# --- Add date column ---\n",
        "today = datetime.now()\n",
        "ghi_df['date'] = today.strftime(\"%Y-%m-%d\")\n",
        "\n",
        "# --- Overwrite the CSV with the current run ---\n",
        "ghi_df.to_csv(one_time_file, index=False)\n",
        "print(f\"✅ Predicted load saved for {today.strftime('%Y-%m-%d')} to {one_time_file}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "i8Yury-LH-MZ",
        "outputId": "f3d89ede-3f91-4e90-c447-fdbaec2e096b"
      },
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n",
            "✅ Predicted load saved for 2025-10-09 to /content/drive/MyDrive/predicted_load.csv\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}